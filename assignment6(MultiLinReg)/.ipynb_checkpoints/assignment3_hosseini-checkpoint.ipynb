{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "599b15ca",
   "metadata": {
    "id": "599b15ca"
   },
   "source": [
    "You are tasked with developing a Neural Network (NN) model to predict the gcPBM TF-DNA\n",
    "binding affinity for three TFs: Max, Mad, and Myc, using a given DNA sequence, similar to your\n",
    "approach in Assignment 1. In this task, we will exclusively utilize 1-mer features. Implement\n",
    "10-fold cross-validation to determine the average r-squared value. [2pt]\n",
    "\n",
    "Hint: Your NN should include a minimum of two hidden layers equipped with an adequate\n",
    "number of nodes. For the final layer, integrate a Dense(1) unit followed by a sigmoid activation\n",
    "function. Opt for mean squared error (mse) as your loss function, utilize the Adam optimizer, and\n",
    "apply the ‘R2Score’ metric to ascertain the r-squared value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ad2171",
   "metadata": {
    "id": "a6ad2171"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 18:49:20.985206: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-15 18:49:21.010495: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-15 18:49:21.010515: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-15 18:49:21.011127: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-15 18:49:21.015644: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-15 18:49:21.508412: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "688d4582",
   "metadata": {
    "id": "688d4582"
   },
   "outputs": [],
   "source": [
    "def load_data(path, header=None):\n",
    "    if header == None:\n",
    "        df = pd.read_table(path, delimiter='\\t', header = header, names = ['sequence', 'affinity'])\n",
    "    elif header == 1:\n",
    "        df = pd.read_table(path, delimiter='\\t').rename(columns={\"SymmetrizedAffinity\": \"affinity\", \"Kmer\":\"sequence\"})\n",
    "        df = df.drop(columns=['idx'])\n",
    "    elif header == 2:\n",
    "        df = pd.read_table(path, delimiter='\\t').rename(columns={\"SymmetrizedAffinity\": \"affinity\", \"Kmer\":\"sequence\"})\n",
    "        df = df.drop(columns=['idx'])\n",
    "    df['affinity'] = df['affinity'].astype(float)\n",
    "    return df\n",
    "\n",
    "max_df = load_data('Max.txt')\n",
    "mad_df = load_data('Mad.txt')\n",
    "myc_df = load_data('Myc.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ccd365",
   "metadata": {
    "id": "b3ccd365"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode_1mer(seq):\n",
    "    temp = list(seq.replace('A', '1000').replace('C', '0100').replace('G', '0010').replace('T','0001'))\n",
    "    return [int(x) for x in temp]\n",
    "\n",
    "def one_hot_encode_2mer(seq):\n",
    "    str = ''\n",
    "    for i in range(len(seq)-1):\n",
    "        str += seq[i:i+2].replace('AA', '1000000000000000').replace('AC', '0100000000000000').replace('AG', '0010000000000000').replace('AT', '0001000000000000').replace('CA', '0000100000000000').replace('CC', '0000010000000000').replace('CG', '0000001000000000').replace('CT', '0000000100000000').replace('GA', '0000000010000000').replace('GC', '0000000001000000').replace('GG', '0000000000100000').replace('GT', '0000000000010000').replace('TA', '0000000000001000').replace('TC', '0000000000000100').replace('TG', '0000000000000010').replace('TT', '0000000000000001')\n",
    "    return [int(x) for x in list(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc957ecb",
   "metadata": {
    "id": "dc957ecb"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df, encoding='1mer', normalize=True):\n",
    "    if normalize == True:\n",
    "        min_val = min(df['affinity'])\n",
    "        max_val = max(df['affinity'])\n",
    "        df['affinity'] = df['affinity'].apply(lambda x: (x - min_val)/(max_val - min_val))\n",
    "    sequences = df['sequence'].to_list()\n",
    "    affinities = df['affinity'].to_list()\n",
    "    if encoding == '1mer':\n",
    "        X = np.array([one_hot_encode_1mer(x) for x in sequences])\n",
    "    else:\n",
    "        X = np.array([one_hot_encode_2mer(x) for x in sequences])\n",
    "    y = np.array(affinities)\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    return X, y, cv\n",
    "\n",
    "max_X, max_y, max_cv = preprocess_data(max_df, '1mer')\n",
    "mad_X, mad_y, mad_cv = preprocess_data(mad_df, '1mer')\n",
    "myc_X, myc_y, myc_cv = preprocess_data(myc_df, '1mer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "qYEZs8TVnZx3",
   "metadata": {
    "id": "qYEZs8TVnZx3"
   },
   "outputs": [],
   "source": [
    "def train_nn_model(X, y, cv, learning_rate, epochs):\n",
    "    r_squared_values = []\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model = None\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape =(X_train.shape[1])))\n",
    "        model.add(Dense(500))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(500))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(1))\n",
    "        # model.add(Activation('sigmoid'))\n",
    "        model.compile(loss='MeanSquaredError',\n",
    "             optimizer=Adam(learning_rate=learning_rate),\n",
    "             metrics=['R2Score'])\n",
    "        model.fit(X_train, y_train, batch_size=100, epochs=epochs, verbose=None)\n",
    "        score = model.evaluate(X_test, y_test)\n",
    "        r_squared_values.append(score[1])\n",
    "    return np.mean(r_squared_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35dd1884",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "35dd1884",
    "outputId": "3e742681-373f-4112-de50-9816823110bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 1ms/step - loss: 0.0027 - r2_score: 0.8994\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.0026 - r2_score: 0.9112\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.0026 - r2_score: 0.9153\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.0022 - r2_score: 0.9233\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.0028 - r2_score: 0.8894\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.0025 - r2_score: 0.9158\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.0027 - r2_score: 0.9112\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.0027 - r2_score: 0.9034\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.0025 - r2_score: 0.9106\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.0028 - r2_score: 0.9088\n",
      "Average r-squared for Max: 0.908852505683899\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0032 - r2_score: 0.9226\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0027 - r2_score: 0.9419\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0028 - r2_score: 0.9428\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0030 - r2_score: 0.9290\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0031 - r2_score: 0.9296\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0027 - r2_score: 0.9390\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0030 - r2_score: 0.9314\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0028 - r2_score: 0.9397\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0031 - r2_score: 0.9344\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0027 - r2_score: 0.9400\n",
      "Average r-squared for Mad: 0.9350336194038391\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0046 - r2_score: 0.8771\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0046 - r2_score: 0.8762\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0042 - r2_score: 0.8826\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0041 - r2_score: 0.8849\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0046 - r2_score: 0.8659\n",
      "22/22 [==============================] - 0s 973us/step - loss: 0.0040 - r2_score: 0.8909\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0041 - r2_score: 0.8797\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0044 - r2_score: 0.8782\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0044 - r2_score: 0.8813\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0043 - r2_score: 0.8839\n",
      "Average r-squared for Myc: 0.8800765872001648\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average r-squared for Max: {train_nn_model(max_X, max_y, max_cv, 0.001, 50)}\")    \n",
    "print(f\"Average r-squared for Mad: {train_nn_model(mad_X, mad_y, mad_cv, 0.001, 50)}\")\n",
    "print(f\"Average r-squared for Myc: {train_nn_model(myc_X, myc_y, myc_cv, 0.001, 50)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb56945c",
   "metadata": {
    "id": "bb56945c"
   },
   "source": [
    "Compare the performance of the neural network using 1-mer features against the linear\n",
    "regression models that utilize both 1-mer and 2-mer encodings for Max, Mad, and Myc. Discuss\n",
    "your observations on their performance. Specifically, analyze why the neural network model\n",
    "with 1-mer data yields satisfactory outcomes, offering an explanation for this observation. [1pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfce62db",
   "metadata": {
    "id": "c837f9f0"
   },
   "source": [
    "**The performance for the 1-mer neural network models for Mad, Max, and Myc were all superior to their respective 1-mer and 2-mer linear regression models. The Mad neural net (NN) model yielded R^2 of 0.93 compared to 0.77 and 0.85 for the 1-mer and 2-mer linear regression counterparts. The Max NN model had R^2 of 0.87 compared to 0.78 and 0.85 for the 1-mer and 2-mer linear regression counterparst. The Myc NN model yielded R^2 of 0.84 compared to the 0.77 and 0.83 from the 1-mer and 2-mer linear regression counterparts. It is impressive to see that the 1-mer neural network model can perform comparably to the 2-mer linear regression model because we would expect that the 2-mer encoding would be a more informative way to represent sequence data. Possible reasons for why a neural network model can outperform linear regression is that it contains numerous hidden layers which can extract more features compared to classical linear regression. Furthermore, the NN model uses sigmoid activation functions whereas classical linear regression may not. Also, the gradient descent calculations may differ between the NN model and classical linear regression (since NN uses Adam optimizer), and NN models also incorporate backpropagation which may be efficient at finding the absolute minimum. Neural networks may perform better at capturing non-linear trends compared to classical linear regression too.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df06ab1",
   "metadata": {
    "id": "9df06ab1"
   },
   "source": [
    "Create a function capable of encoding 1-mer and 2-mer sequences, including sequences with\n",
    "5-methylcytosine, denoted as ‘M’. [2pt]\n",
    "\n",
    "Hint: For 1-mer encoding, use the following representations: A as 10000, C as 01000, G as 00100,\n",
    "T as 00010, and M as 00001. For 2-mer encoding, start with AA represented as\n",
    "1000000000000000000000000, proceed through combinations like AC as\n",
    "0100000000000000000000000, and conclude with MM as 0000000000000000000000001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7121b9c",
   "metadata": {
    "id": "c7121b9c"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode_1mer(seq):\n",
    "    temp = list(seq.replace('A', '10000').replace('C', '01000').replace('G', '00100').replace('T','00010').replace('M', '00001'))\n",
    "    return [int(x) for x in temp]\n",
    "\n",
    "def one_hot_encode_2mer(seq):\n",
    "    encoding = ''\n",
    "    encoding_map_2mer = {\n",
    "    'AA': '1000000000000000000000000', 'AC': '0100000000000000000000000', 'AG': '0010000000000000000000000',\n",
    "    'AT': '0001000000000000000000000', 'AM': '0000100000000000000000000', 'CA': '0000010000000000000000000',\n",
    "    'CC': '0000001000000000000000000', 'CG': '0000000100000000000000000', 'CT': '0000000010000000000000000',\n",
    "    'CM': '0000000001000000000000000', 'GA': '0000000000100000000000000', 'GC': '0000000000010000000000000',\n",
    "    'GG': '0000000000001000000000000', 'GT': '0000000000000100000000000', 'GM': '0000000000000010000000000',\n",
    "    'TA': '0000000000000001000000000', 'TC': '0000000000000000100000000', 'TG': '0000000000000000010000000',\n",
    "    'TT': '0000000000000000001000000', 'TM': '0000000000000000000100000', 'MA': '0000000000000000000010000',\n",
    "    'MC': '0000000000000000000001000', 'MG': '0000000000000000000000100', 'MT': '0000000000000000000000010',\n",
    "    'MM': '0000000000000000000000001'\n",
    "    }\n",
    "    for i in range(len(seq) - 1):\n",
    "        dinucleotide = seq[i:i + 2]\n",
    "        encoding += encoding_map_2mer[dinucleotide]\n",
    "    return [int(x) for x in list(encoding)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3104cc0",
   "metadata": {
    "id": "a3104cc0"
   },
   "source": [
    "Load and encode the EpiSelex-seq data for the TFs Atf4 (Atf4.txt) and Cebpb (Cebpb.txt). Apply\n",
    "the neural network model you developed in Question 1 and linear regression models using\n",
    "1-mer and 2-mer features to predict their binding affinity to both methylated and unmethylated\n",
    "DNA sequences. Note that the binding data is unaligned. [2pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ed661b6",
   "metadata": {
    "id": "1ed661b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 21:35:52.155091: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f5b6c008de0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-13 21:35:52.155129: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-02-13 21:35:52.176442: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1707888952.232231   11380 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-02-13 21:35:52.233226: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
      "2024-02-13 21:35:52.236824: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
      "2024-02-13 21:35:52.236899: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
      "2024-02-13 21:35:52.236978: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
      "2024-02-13 21:36:13.435979: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/173 [==============================] - 0s 957us/step - loss: 3.2289e-05 - r2_score: 0.7959\n",
      "173/173 [==============================] - 0s 965us/step - loss: 1.3182e-05 - r2_score: 0.8903\n",
      "173/173 [==============================] - 0s 939us/step - loss: 1.5394e-05 - r2_score: 0.9568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 21:58:49.887477: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/173 [==============================] - 0s 947us/step - loss: 5.6615e-05 - r2_score: 0.7909\n",
      "173/173 [==============================] - 0s 943us/step - loss: 7.2805e-06 - r2_score: 0.9214\n",
      "173/173 [==============================] - 0s 960us/step - loss: 1.4934e-05 - r2_score: 0.9462\n",
      "173/173 [==============================] - 0s 959us/step - loss: 2.9599e-05 - r2_score: 0.9248\n",
      "173/173 [==============================] - 0s 999us/step - loss: 4.2465e-05 - r2_score: 0.7581\n",
      "173/173 [==============================] - 0s 961us/step - loss: 7.1975e-06 - r2_score: 0.9490\n",
      "173/173 [==============================] - 0s 963us/step - loss: 1.8094e-05 - r2_score: 0.9155\n",
      "Average Neural Net r-squared for Atf4 1-mer: 0.8848921656608582\n"
     ]
    }
   ],
   "source": [
    "atf_df = load_data('Atf4.txt', header=1)\n",
    "atf_X, atf_y, atf_cv = preprocess_data(atf_df, '1mer', normalize=True)\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    print(f\"Average Neural Net r-squared for Atf4 1-mer: {train_nn_model(atf_X, atf_y, atf_cv, 0.001, 300)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d302730b",
   "metadata": {
    "id": "d302730b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/173 [==============================] - 1s 2ms/step - loss: 6.6614e-05 - r2_score: 0.5790\n",
      "173/173 [==============================] - 1s 3ms/step - loss: 3.6134e-06 - r2_score: 0.9699\n",
      "173/173 [==============================] - 1s 2ms/step - loss: 2.2974e-05 - r2_score: 0.9355\n",
      "173/173 [==============================] - 1s 2ms/step - loss: 1.7304e-05 - r2_score: 0.9361\n",
      "173/173 [==============================] - 1s 2ms/step - loss: 4.8075e-06 - r2_score: 0.9481\n",
      "173/173 [==============================] - 1s 2ms/step - loss: 1.6504e-05 - r2_score: 0.9405\n",
      "173/173 [==============================] - 1s 2ms/step - loss: 7.6570e-06 - r2_score: 0.9805\n",
      "173/173 [==============================] - 0s 1ms/step - loss: 7.4230e-06 - r2_score: 0.9577\n",
      "173/173 [==============================] - 0s 1ms/step - loss: 2.8168e-05 - r2_score: 0.8003\n",
      "173/173 [==============================] - 0s 1ms/step - loss: 3.2609e-05 - r2_score: 0.8477\n",
      "Average Neural Net r-squared for Atf4 2-mer: 0.8895444989204406\n"
     ]
    }
   ],
   "source": [
    "atf_df = load_data('Atf4.txt', header=1)\n",
    "atf_X, atf_y, atf_cv = preprocess_data(atf_df, '2mer', normalize=True)\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    print(f\"Average Neural Net r-squared for Atf4 2-mer: {train_nn_model(atf_X, atf_y, atf_cv, 0.001, 300)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cca9ffd1",
   "metadata": {
    "id": "cca9ffd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 1ms/step - loss: 7.8184e-05 - r2_score: 0.9696\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 4.2165e-05 - r2_score: 0.9838\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 3.4222e-05 - r2_score: 0.9855\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 8.6573e-05 - r2_score: 0.9655\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 5.6756e-05 - r2_score: 0.9786\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 3.4405e-05 - r2_score: 0.9859\n",
      "113/113 [==============================] - 0s 993us/step - loss: 6.6314e-05 - r2_score: 0.9694\n",
      "113/113 [==============================] - 0s 999us/step - loss: 5.4486e-05 - r2_score: 0.9780\n",
      "113/113 [==============================] - 0s 991us/step - loss: 2.6217e-05 - r2_score: 0.9874\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 3.6380e-05 - r2_score: 0.9862\n",
      "Average Neural Net r-squared for Cebpb 1-mer: 0.9789856851100922\n"
     ]
    }
   ],
   "source": [
    "ceppb_df = load_data('Cebpb.txt', header=2)\n",
    "ceppb_X, ceppb_y, ceppb_cv = preprocess_data(ceppb_df, '1mer', normalize=True)\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    print(f\"Average Neural Net r-squared for Cebpb 1-mer: {train_nn_model(ceppb_X, ceppb_y, ceppb_cv, 0.001, 300)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19396d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 1ms/step - loss: 2.7785e-05 - r2_score: 0.9892\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 2.4337e-05 - r2_score: 0.9906\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 1.2924e-04 - r2_score: 0.9451\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 6.2368e-05 - r2_score: 0.9751\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 3.8989e-05 - r2_score: 0.9853\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 4.6038e-05 - r2_score: 0.9811\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 3.1589e-05 - r2_score: 0.9854\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 3.4024e-05 - r2_score: 0.9863\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 3.3683e-05 - r2_score: 0.9838\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 2.3656e-05 - r2_score: 0.9910\n",
      "Average Neural Net r-squared for Cebpb 2-mer: 0.9813009738922119\n"
     ]
    }
   ],
   "source": [
    "ceppb_df = load_data('Cebpb.txt', header=2)\n",
    "ceppb_X, ceppb_y, ceppb_cv = preprocess_data(ceppb_df, '2mer', normalize=True)\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    print(f\"Average Neural Net r-squared for Cebpb 2-mer: {train_nn_model(ceppb_X, ceppb_y, ceppb_cv, 0.001, 300)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d630a7b7-9747-4449-9c24-aa80dec0d11a",
   "metadata": {
    "id": "d630a7b7-9747-4449-9c24-aa80dec0d11a"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f134e824-1731-4749-95ef-37c6457b4f5b",
   "metadata": {
    "id": "f134e824-1731-4749-95ef-37c6457b4f5b",
    "outputId": "74bad827-3e63-4c95-c1d7-78b12c5c9886"
   },
   "outputs": [],
   "source": [
    "def train_linreg_model(X, y, cv):\n",
    "    # results\n",
    "    mse_scores = []; r_squared_scores = []\n",
    "\n",
    "    # Initialize Linear Regression model\n",
    "    #model = LinearRegression()\n",
    "    #model = Lasso(alpha=0.001)\n",
    "    #model = Ridge(alpha=0.001)\n",
    "    model = ElasticNet(alpha=0.001, l1_ratio=0.5)\n",
    "\n",
    "    # Loop over each fold\n",
    "    for train_index, valid_index in cv.split(X):\n",
    "        # Split data\n",
    "        X_train, X_valid = X[train_index], X[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on valid set\n",
    "        predictions = model.predict(X_valid)\n",
    "\n",
    "        # Evaluate the model\n",
    "        mse = mean_squared_error(y_valid, predictions)\n",
    "        mse_scores.append(mse)\n",
    "\n",
    "        r_squared = model.score(X_valid, y_valid)\n",
    "        r_squared_scores.append(r_squared)\n",
    "\n",
    "    average_mse = np.mean(mse_scores)\n",
    "    average_r_squared = np.mean(r_squared_scores)\n",
    "    return average_r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "555ca62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Linear Regression r-squared for Atf4 1-mer: -0.00013149165587649224\n",
      "Average Linear Regression r-squared for Cebpb 1-mer: 0.09966749707502714\n"
     ]
    }
   ],
   "source": [
    "ceppb_df = load_data('Cebpb.txt', header=2)\n",
    "ceppb_X, ceppb_y, ceppb_cv = preprocess_data(ceppb_df, '1mer', normalize=True)\n",
    "\n",
    "atf_df = load_data('Atf4.txt', header=1)\n",
    "atf_X, atf_y, atf_cv = preprocess_data(atf_df, '1mer', normalize=True)\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    print(\"Average Linear Regression r-squared for Atf4 1-mer: \" + str(train_linreg_model(atf_X, atf_y, atf_cv)))\n",
    "    print(\"Average Linear Regression r-squared for Cebpb 1-mer: \" + str(train_linreg_model(ceppb_X, ceppb_y, ceppb_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b289be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Linear Regression r-squared for Atf4 2-mer: -0.00013149165587649224\n",
      "Average Linear Regression r-squared for Cebpb 2-mer: 0.12787750432604433\n"
     ]
    }
   ],
   "source": [
    "ceppb_df = load_data('Cebpb.txt', header=2)\n",
    "ceppb_X, ceppb_y, ceppb_cv = preprocess_data(ceppb_df, '2mer', normalize=True)\n",
    "\n",
    "atf_df = load_data('Atf4.txt', header=1)\n",
    "atf_X, atf_y, atf_cv = preprocess_data(atf_df, '2mer', normalize=True)\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    print(\"Average Linear Regression r-squared for Atf4 2-mer: \" + str(train_linreg_model(atf_X, atf_y, atf_cv)))\n",
    "    print(\"Average Linear Regression r-squared for Cebpb 2-mer: \" + str(train_linreg_model(ceppb_X, ceppb_y, ceppb_cv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c005ca7",
   "metadata": {},
   "source": [
    "Compare the results and elucidate why the most effective model outperforms the other two in\n",
    "the context of this type of experimental data. [1pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c3d1ec",
   "metadata": {},
   "source": [
    "**The neural network models are far more superior than classical linear regression for fitting to the Atf4 and Cebpb methylation data as evidenced by the r-squared values. For Atf4, the 1-mer and 2-mer encodings trained on neural nets yielded R^2 of 0.88 and 0.89 compared to 0.00 obtained from both 1-mer and 2-mer linear regression models. For Cebpb, the 1-mer and 2-mer neural network models yielded R^2 of 0.97 and 0.98 compared to 0.10 and 0.13 for the linear regression model respectively. The 2-mer neural network model appears to be the best model in terms of fit, although the 1-mer neural network model is comparable. Overall, the neural network models were both superior to their classical linear regression counterparts because they may be more suited to unaligned sequences. While linear regression may require extensive data preprocessing in order to align the sequences and represent binding data appropriately, neural networks are more flexible with regards to data input and they are clearly able to extract features effectively despite the unalignment of the data. The 2-mer encoding is slightly superior to 1-mer because TF binding is modulated by the electrophysical and steric properties of DNA in addition to the raw sequence of nucleotides, so an encoding consisting of neighboring nucleotides better captures the interactions between multiple bases to form an ideal binding pocket.**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "machineLearning",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
